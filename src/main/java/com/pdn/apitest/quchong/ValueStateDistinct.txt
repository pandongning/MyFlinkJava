package com.pdn.quchong;

import com.pdn.utils.ExecutionEnvironmentUtils;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.functions.RichFlatMapFunction;
import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.api.common.state.MapStateDescriptor;
import org.apache.flink.api.common.state.StateTtlConfig;
import org.apache.flink.api.common.state.ValueState;
import org.apache.flink.api.common.state.ValueStateDescriptor;
import org.apache.flink.api.common.time.Time;
import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.connector.kafka.source.KafkaSource;
import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.datastream.KeyedStream;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.util.Collector;

/**
 * @author pdn
 */
public class ValueStateDistinct {
    public static void main(String[] args) throws Exception {

        StreamExecutionEnvironment env = ExecutionEnvironmentUtils.getExecutionEnvironment();
        env.setParallelism(3);

        KafkaSource<String> source = KafkaSource.<String>builder()
//                因为本地配置了端口的映射，所以此处用于本地测试
                .setBootstrapServers("127.0.0.1:9094")
//                集群环境的配置
//                .setBootstrapServers("slave1:9094,slave2:9092,master:9093")
                .setTopics("a")
                .setGroupId("my-group-a")
                .setStartingOffsets(OffsetsInitializer.latest())
                .setValueOnlyDeserializer(new SimpleStringSchema())
                .build();

        DataStreamSource<String> streamSource = env.fromSource(source, WatermarkStrategy.noWatermarks(), "a");
        SingleOutputStreamOperator<Tuple2<String, String>> map = streamSource.map(x -> new Tuple2<String, String>(x.split(",")[0], x.split(",")[1]))
                .returns(Types.TUPLE(Types.STRING,Types.STRING));

//        下面必须进行keyBy。因为去重是根据key进行去重的，
        KeyedStream<Tuple2<String, String>, String> keyedStream = map.keyBy(x -> x.f0);

        SingleOutputStreamOperator<String> quchong = keyedStream.flatMap(new RichFlatMapFunction<Tuple2<String, String>, String>() {
        /**
        此处则有一个十分重要的知识点，对于keyBy之后的算子到底有多少并行度。
        假如我们的任务的并行度为3，则此处的flatMap会有3个实例。则下面的open函数会执行三次。
        注意虽然此处的open函数只会执行三次，
        但是在最后实际执行的时候，对于里面的的状态变量ValueState。却是不同的key在执行的时候会初始化自己该key对应的ValueState。而不是该三个flatMap。对应三个ValueState。
        所以该任务是不同的key，会创建这个key自己对应的flatMap实例。每个不同的key对应的状态ValueState是不同的.
        然后当相同的key再次来临的时候，其会进入到这个key自己的latMap实例进行计算
        */

            ValueState<Boolean> keyHasBeenSeen;

            //           每个算字都会执行一次，比如并行度为3，则此时会执行3次
            @Override
            public void open(Configuration parameters) throws Exception {
                System.out.println("我又执行一次呢");
                super.open(parameters);
                StateTtlConfig build = StateTtlConfig.newBuilder(Time.seconds(5))
                        .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)
                        .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)
                        .build();
                ValueStateDescriptor<Boolean> hasBeenSeen = new ValueStateDescriptor<>("keyHasBeenSeen", Boolean.class);
                keyHasBeenSeen = getRuntimeContext().getState(hasBeenSeen);

            }

            @Override
            public void flatMap(Tuple2<String, String> value, Collector<String> out) throws Exception {
                if (keyHasBeenSeen.value() == null) {
                    out.collect(value.f0+"$$"+value.f1);
                    keyHasBeenSeen.update(true);
                }

            }
        }).returns(Types.STRING);


        quchong.print("quchong");


        env.execute("KafkaConnector");
    }
}
